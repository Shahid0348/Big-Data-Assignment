# -*- coding: utf-8 -*-
"""Awais.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LxaXsY8CWPf1-imsFyLg8Vz0FH9aFhdr
"""

# prompt: data analize by using py spark and install library of pyspark

!pip install pyspark

# prompt: write a code to apply pyspark library function on data set
# data set we provide you
# apply all operation on this data set that are available in the pyspark library

from pyspark.sql import SparkSession
from pyspark.sql.functions import *

spark = SparkSession.builder.appName('pyspark_example').getOrCreate()

# Load the data set
df = spark.read.csv('/content/sample_data/2018-2019_Daily_Attendance_20240429.csv', header=True)

# Print the schema of the data set
df.printSchema()

# Show the first few rows of the data set
df.show(5)

# Select specific columns
df.select('School DBN', 'Date').show()

# Filter the data set
df.filter(df['Absent'] > 10).show()

# Group the data set
df.groupBy('Present').count().show()

# Join two data sets
df1 = df.select('Absent', 'Present')
df2 = df.select('School DBN', 'Enrolled')


# Perform aggregations
df.agg({'Present': 'max', 'Absent': 'min'}).show()

# Use SQL statements
df.createOrReplaceTempView('table_name')
spark.sql('SELECT * FROM table_name WHERE Absent < 10').show()

# prompt: write a code to apply more operation on this data set using pyspark

# Create a new column that calculates the percentage of attendance for each row
df = df.withColumn('Attendance_Percentage', (df['Present'] / df['Enrolled']) * 100)

# Show the first few rows of the new data set
df.show(5)

# Group the data set by school DBN and calculate the average attendance percentage for each school
df.groupBy('School DBN').agg(avg('Attendance_Percentage').alias('Average Attendance Percentage')).show()

# Filter the data set to only include rows where the attendance percentage is greater than 90%
df.filter(df['Attendance_Percentage'] > 90).show()

# Join the data set with another data set that contains information about the schools
school_info_df = spark.read.csv('/content/sample_data/2018-2019_Daily_Attendance_20240429.csv', header=True)
df_joined = df.join(school_info_df, on='School DBN', how='left')

# Show the first few rows of the joined data set
df_joined.show(5)

# prompt: write a code
# which student have absent less then 60 percent
# to show this in  columns formate

df_less_than_60_percent = df.filter(df['Absent'] < (df['Enrolled'] * 0.6))
df_less_than_60_percent.select('School DBN', 'Absent', 'Enrolled').show()

